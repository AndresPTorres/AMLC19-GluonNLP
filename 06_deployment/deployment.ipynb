{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment with TVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this notebook, we will focus on the deployment with TVM. [TVM](https://tvm.ai) is an open deep learning compiler for CPUs, GPUs, and specialized accelerators. [Amazon Sagemaker Neo](https://aws.amazon.com/sagemaker/neo/) provides the compilation service that built on top of TVM.\n",
    "\n",
    "In this tutorial, we use BERT model for the question answering task to show how deployment through TVM works. Specifically, we will:\n",
    "\n",
    "- learn how to convert MXNet model to TVM representation (Relay)\n",
    "- learn how to compile and run TVM model, and\n",
    "- evaluate TVM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We first import the MXNet, GluonNLP, and TVM libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.416134Z",
     "start_time": "2019-06-14T01:45:26.339759Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections, time, logging\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import bert\n",
    "import qa_utils\n",
    "from bert.bert_qa_evaluate import PredResult, predict\n",
    "from bert.export.hybrid_bert import HybridBERTForQA, get_hybrid_model\n",
    "from bert.data.qa import SQuADTransform, preprocess_dataset\n",
    "# TVM libraries\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm import autotvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load hybrid BERT model\n",
    "\n",
    "In order to export the model for deployment, we need to use hybrid model with fixed sequence length. Here we specify the maximum sequence length to 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.715444Z",
     "start_time": "2019-06-14T01:45:27.569118Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded checkpoint to ./temp/bert_qa-7eb11865.params\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 256\n",
    "base_model, vocab = get_hybrid_model(\n",
    "    name=\"bert_12_768_12\",\n",
    "    dataset_name=\"book_corpus_wiki_en_uncased\",\n",
    "    pretrained=False,\n",
    "    use_pooler=False,\n",
    "    use_decoder=False,\n",
    "    use_classifier=False,\n",
    "    seq_length=max_seq_length)\n",
    "net = HybridBERTForQA(base_model)\n",
    "mx_ctx = mx.cpu()\n",
    "ckpt = qa_utils.download_qa_ckpt()\n",
    "net.load_parameters(ckpt, ctx=mx_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prepare the sample dataset\n",
    "\n",
    "Similar to the questiong answering tutorial, we use the SQuAD dataset and create a subset dataset with 10 samples for demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.555133Z",
     "start_time": "2019-06-14T01:45:27.418706Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Transform dataset costs 0.22 seconds.\n"
     ]
    }
   ],
   "source": [
    "full_data = nlp.data.SQuAD(segment='dev', version='1.1')\n",
    "\n",
    "# loading a subset of the dev set of SQuAD\n",
    "num_target_samples = 10\n",
    "target_samples = [full_data[i] for i in range(num_target_samples)]\n",
    "dataset = mx.gluon.data.SimpleDataset(target_samples)\n",
    "\n",
    "tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=True)\n",
    "transform = bert.data.qa.SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "dev_data_transform, _ = bert.data.qa.preprocess_dataset(dataset, transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then convert the transformed texts to subword indices and prepare the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "    indices = vocab[subwords]\n",
    "    return example_id, indices, type_ids, length, start, end\n",
    "dev_data_transform = dev_data_transform.transform(vocab_lookup, lazy=False)\n",
    "\n",
    "batch_size = 1\n",
    "dev_dataloader = mx.gluon.data.DataLoader(\n",
    "    dev_data_transform, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compile MXNet model with TVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we convert the MXNet model into Relay. We need to provide a mapping from input names to their shapes at this step. TVM frontend converter supports both MXNet static graph (symbol) and HybridBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_dict = {\n",
    "    'data0': (1, max_seq_length), # inputs\n",
    "    'data1': (1, max_seq_length), # token types\n",
    "    'data2': (1,) # sequence length\n",
    "}\n",
    "mod, params = relay.frontend.from_mxnet(net, shape_dict)\n",
    "# uncomment the following line to see the converted model in Relay IR\n",
    "# print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load the AutoTVM logs and build module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the AutoTVM logs that were previously tuned on c5.9x instances.\n",
    "\n",
    "We won't cover how to tune kernels using AutoTVM in this tutorial. If you are interested, you can check the [auto tuning tutorial](https://docs.tvm.ai/tutorials/autotvm/tune_relay_x86.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"autotvm_logs\"\n",
    "logs = [os.path.join(log_dir, f) for f in os.listdir(log_dir)]\n",
    "autotvm_ctx = autotvm.apply_history_best(None)\n",
    "for log_file in logs:\n",
    "    autotvm_ctx.load(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then compile the model. We specify the target CPU as skylake avx512 to use the vectorized instructions for floating operations. \n",
    "\n",
    "If compile on other devices, e.g., ARM CPU, we need to change the target string, e.g., \"llvm -device=arm_cpu -target=aarch64-linux-gnu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"llvm -mcpu=skylake-avx512\"\n",
    "# change the target when compile on ARM CPU\n",
    "# target = \"llvm -device=arm_cpu -target=aarch64-linux-gnu\"\n",
    "with autotvm_ctx:\n",
    "    with relay.build_config(opt_level=3):\n",
    "        graph, lib, params = relay.build(mod[mod.entry_func], target, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Export library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can export library, graph structure, and parameters into files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.export_library(\"deploy_lib.tar\")\n",
    "with open(\"deploy_graph.json\", \"w\") as fo:\n",
    "    fo.write(graph)\n",
    "with open(\"deploy_param.params\", \"wb\") as fo:\n",
    "    fo.write(relay.save_param_dict(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate TVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now load back graph, library, and params from files that are exported earlier, and create the graph runtime to execute the compiled graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm.contrib.graph_runtime as runtime\n",
    "\n",
    "loaded_graph = open(\"deploy_graph.json\").read()\n",
    "loaded_lib = tvm.module.load(\"deploy_lib.tar\")\n",
    "loaded_params = bytearray(open(\"deploy_param.params\", \"rb\").read())\n",
    "\n",
    "tvm_ctx = tvm.cpu()\n",
    "ex = runtime.create(loaded_graph, loaded_lib, tvm_ctx)\n",
    "ex.load_params(loaded_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that hybrid BERT model requires fixed length inputs. Therefore, before we feed in the input and token types, we need to pad them to the max sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(arr, length, pad_val, dtype=\"float32\"):\n",
    "    padded = np.full(shape=(1, length), fill_value=pad_val, dtype=dtype)\n",
    "    padded[0, :arr.shape[1]] = arr.asnumpy()[0]\n",
    "    return padded\n",
    "\n",
    "tvm_results = collections.defaultdict(list)\n",
    "example_ids, inputs, token_types, valid_length, _, _ = next(iter(dev_dataloader))\n",
    "padded_inputs = pad(inputs, max_seq_length, vocab[vocab.padding_token])\n",
    "padded_token_types = pad(token_types, max_seq_length, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's run the graph runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ex.set_input(data0=padded_inputs,\n",
    "             data1=padded_token_types,\n",
    "             data2=valid_length.astype('float32').asnumpy())\n",
    "ex.run()\n",
    "out = ex.get_output(0)\n",
    "output = np.split(out.asnumpy(), axis=2, indices_or_sections=2)\n",
    "example_ids = example_ids.asnumpy().tolist()\n",
    "pred_start = output[0].reshape((1, -1))\n",
    "pred_end = output[1].reshape((1, -1))\n",
    "\n",
    "for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "    tvm_results[example_id].append(PredResult(start=start, end=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.36% \t Denver Broncos\n",
      "0.23% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.20% \t Broncos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_utils.predict(dataset, tvm_results, vocab, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also execute MXNet to check the correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_results = collections.defaultdict(list)\n",
    "example_ids, inputs, token_types, valid_length, _, _ = next(iter(dev_dataloader))\n",
    "padded_inputs = pad(inputs, max_seq_length, vocab[vocab.padding_token])\n",
    "padded_token_types = pad(token_types, max_seq_length, 0)\n",
    "\n",
    "out = net(mx.nd.array(padded_inputs, mx_ctx),\n",
    "          mx.nd.array(padded_token_types, mx_ctx),\n",
    "          valid_length.astype('float32').as_in_context(mx_ctx))\n",
    "output = mx.nd.split(out, axis=2, num_outputs=2)\n",
    "example_ids = example_ids.asnumpy().tolist()\n",
    "pred_start = output[0].reshape((0, -3)).asnumpy()\n",
    "pred_end = output[1].reshape((0, -3)).asnumpy()\n",
    "for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "    mx_results[example_id].append(PredResult(start=start, end=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.36% \t Denver Broncos\n",
      "0.23% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.20% \t Broncos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_utils.predict(dataset, mx_results, vocab, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benchmark TVM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmark shows the mean inference time of TVM and MXNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM mean inference time: 93.56 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = np.random.uniform(size=(1, max_seq_length)).astype('float32')\n",
    "token_types = np.random.uniform(size=(1, max_seq_length)).astype('float32')\n",
    "valid_length = np.asarray([max_seq_length]).astype('float32')\n",
    "ex.set_input(data0=inputs, data1=token_types, data2=valid_length)\n",
    "\n",
    "ftimer = ex.module.time_evaluator(\"run\", tvm_ctx, number=10)\n",
    "prof_res = np.array(ftimer().results) * 1000  # convert to millisecond\n",
    "print(\"TVM mean inference time: %.2f ms\" % np.mean(prof_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MXNet mean inference time: 85.90 ms\n"
     ]
    }
   ],
   "source": [
    "inputs_nd = mx.nd.array(inputs)\n",
    "token_types_nd = mx.nd.array(token_types)\n",
    "valid_length_nd = mx.nd.array(valid_length)\n",
    "# dry run\n",
    "mx_out = net(inputs_nd, token_types_nd, valid_length_nd)\n",
    "mx_out.wait_to_read()\n",
    "# benchmark\n",
    "number = 10\n",
    "beg = time.time()\n",
    "for _ in range(number):\n",
    "    mx_out = net(inputs_nd, token_types_nd, valid_length_nd)\n",
    "    mx_out.wait_to_read()\n",
    "lat = (time.time() - beg) * 1e3 / number\n",
    "print('MXNet mean inference time: %.2f ms' % lat)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
