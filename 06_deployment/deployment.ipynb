{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment with TVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this notebook, we will focus on the deployment with TVM. [TVM](https://tvm.ai) is an open deep learning compiler for CPUs, GPUs, and specialized accelerators. [Amazon Sagemaker Neo](https://aws.amazon.com/sagemaker/neo/) provides the compilation service that built on top of TVM.\n",
    "\n",
    "In this tutorial, we use BERT model for the question answering task to show how deployment through TVM works. Specifically, we will:\n",
    "\n",
    "- learn how to convert MXNet model to TVM representation (Relay)\n",
    "- learn how to compile and run TVM model, and\n",
    "- evaluate TVM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the MXNet, GluonNLP, and TVM libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.416134Z",
     "start_time": "2019-06-14T01:45:26.339759Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections, time, logging\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import bert\n",
    "import qa_utils\n",
    "from bert.bert_qa_evaluate import PredResult, predict\n",
    "from bert.export.hybrid_bert import HybridBERTForQA, get_hybrid_model\n",
    "from bert.data.qa import SQuADTransform, preprocess_dataset\n",
    "# TVM libraries\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm import autotvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load hybrid BERT model\n",
    "\n",
    "Currently hybrid BERT model for QA doesn't support control flow and requires fixed sequence length. So we need to specify a maximum sequence length and padded the input at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.715444Z",
     "start_time": "2019-06-14T01:45:27.569118Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded checkpoint to ./temp/bert_qa-7eb11865.params\n"
     ]
    }
   ],
   "source": [
    "from bert.export.hybrid_bert import HybridBERTForQA, get_hybrid_model\n",
    "max_seq_length = 256\n",
    "base_model, vocab = get_hybrid_model(\n",
    "    name=\"bert_12_768_12\",\n",
    "    dataset_name=\"book_corpus_wiki_en_uncased\",\n",
    "    pretrained=False,\n",
    "    use_pooler=False,\n",
    "    use_decoder=False,\n",
    "    use_classifier=False,\n",
    "    seq_length=max_seq_length)\n",
    "\n",
    "net = HybridBERTForQA(base_model)\n",
    "mx_ctx = mx.cpu()\n",
    "ckpt = qa_utils.download_qa_ckpt()\n",
    "net.load_parameters(ckpt, ctx=mx_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the sample dataset\n",
    "\n",
    "Similar to the questiong answering tutorial, we also load the SQuAD dataset. In this tutorial, we create a subset dataset with 20 samples from SQuAD dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.555133Z",
     "start_time": "2019-06-14T01:45:27.418706Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the created dataset subsampled from SQuAD = 20\n",
      "Done! Transform dataset costs 0.30 seconds.\n"
     ]
    }
   ],
   "source": [
    "full_data = nlp.data.SQuAD(segment='dev', version='1.1')\n",
    "\n",
    "# loading a subset of the dev set of SQuAD\n",
    "num_target_samples = 20\n",
    "target_samples = [full_data[i] for i in range(num_target_samples)]\n",
    "dataset = mx.gluon.data.SimpleDataset(target_samples)\n",
    "print('Number of samples in the created dataset subsampled from SQuAD = %d' % len(dataset))\n",
    "\n",
    "tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=True)\n",
    "transform = bert.data.qa.SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "dev_data_transform, _ = bert.data.qa.preprocess_dataset(dataset, transform)\n",
    "\n",
    "def vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "    indices = vocab[subwords]\n",
    "    return example_id, indices, type_ids, length, start, end\n",
    "dev_data_transform = dev_data_transform.transform(vocab_lookup, lazy=False)\n",
    "\n",
    "batch_size = 1\n",
    "dev_dataloader = mx.gluon.data.DataLoader(\n",
    "    dev_data_transform, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile MXNet model with TVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the MXNet model into Relay. We need to provide a mapping from input names to their shapes at this step. TVM frontend converter supports both MXNet static graph (symbol) and HybridBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_dict = {\n",
    "    'data0': (1, max_seq_length), # inputs\n",
    "    'data1': (1, max_seq_length), # token types\n",
    "    'data2': (1,) # sequence length\n",
    "}\n",
    "mod, params = relay.frontend.from_mxnet(net, shape_dict)\n",
    "# uncomment the following line to see the converted model in Relay IR\n",
    "# print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the AutoTVM logs and build module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the AutoTVM logs that were previously tuned on c5.9x instances. TVM uses these tuning results and applies the best schedule during compilation for the target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"autotvm_logs\"\n",
    "logs = [os.path.join(log_dir, f) for f in os.listdir(log_dir)]\n",
    "autotvm_ctx = autotvm.apply_history_best(None)\n",
    "for log_file in logs:\n",
    "    autotvm_ctx.load(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile the model. We specify the target CPU as skylake avx512 in order to use the vectorized instructions for floating operations. If we want to compile on other devices, e.g., ARM CPU, we need to change the target string, e.g., \"llvm -device=arm_cpu -target=aarch64-linux-gnu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"llvm -mcpu=skylake-avx512\"\n",
    "# change the target when compile on ARM CPU\n",
    "# target = \"llvm -device=arm_cpu -target=aarch64-linux-gnu\"\n",
    "with autotvm_ctx:\n",
    "    with relay.build_config(opt_level=3):\n",
    "        graph, lib, params = relay.build(mod[mod.entry_func], target, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the graph runtime to execute the compiled graph. Before we run the executor, we first bind the parameters through `set_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm.contrib.graph_runtime as runtime\n",
    "\n",
    "tvm_ctx = tvm.cpu()\n",
    "ex = runtime.create(graph, lib, tvm_ctx)\n",
    "ex.set_input(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that hybrid BERT model requires fixed length inputs. Therefore, before we feed in the input and token types, we need to pad them to the max sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost=2.41 s, Thoughput=8.31 samples/s\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.36% \t Denver Broncos\n",
      "0.23% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.20% \t Broncos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pad(arr, length, pad_val, dtype=\"float32\"):\n",
    "    padded = np.full(shape=(1, length), fill_value=pad_val, dtype=dtype)\n",
    "    padded[0, :arr.shape[1]] = arr.asnumpy()[0]\n",
    "    return padded\n",
    "\n",
    "tvm_results = collections.defaultdict(list)\n",
    "epoch_tic = time.time()\n",
    "total_num = 0\n",
    "for data in dev_dataloader:\n",
    "    example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "    total_num += len(inputs)\n",
    "    padded_inputs = pad(inputs, max_seq_length, vocab[vocab.padding_token])\n",
    "    padded_token_types = pad(token_types, max_seq_length, 0)\n",
    "    ex.set_input(data0=padded_inputs,\n",
    "                 data1=padded_token_types,\n",
    "                 data2=valid_length.astype('float32').asnumpy())\n",
    "    ex.run()\n",
    "    out = ex.get_output(0)\n",
    "    output = np.split(out.asnumpy(), axis=2, indices_or_sections=2)\n",
    "    example_ids = example_ids.asnumpy().tolist()\n",
    "    pred_start = output[0].reshape((1, -1))\n",
    "    pred_end = output[1].reshape((1, -1))\n",
    "\n",
    "    for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "        tvm_results[example_id].append(PredResult(start=start, end=end))\n",
    "\n",
    "epoch_toc = time.time()\n",
    "print('Time cost={:.2f} s, Thoughput={:.2f} samples/s'.format(\n",
    "    epoch_toc - epoch_tic, total_num/(epoch_toc - epoch_tic)))\n",
    "\n",
    "qa_utils.predict(dataset, tvm_results, vocab, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also execute MXNet to check the correctness and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost=1.79 s, Thoughput=11.18 samples/s\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.36% \t Denver Broncos\n",
      "0.23% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.20% \t Broncos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mx_results = collections.defaultdict(list)\n",
    "\n",
    "epoch_tic = time.time()\n",
    "total_num = 0\n",
    "for data in dev_dataloader:\n",
    "    example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "    total_num += len(inputs)\n",
    "    padded_inputs = pad(inputs, max_seq_length, vocab[vocab.padding_token])\n",
    "    padded_token_types = pad(token_types, max_seq_length, 0)\n",
    "    out = net(mx.nd.array(padded_inputs, mx_ctx),\n",
    "              mx.nd.array(padded_token_types, mx_ctx),\n",
    "              valid_length.astype('float32').as_in_context(mx_ctx))\n",
    "\n",
    "    output = mx.nd.split(out, axis=2, num_outputs=2)\n",
    "    example_ids = example_ids.asnumpy().tolist()\n",
    "    pred_start = output[0].reshape((0, -3)).asnumpy()\n",
    "    pred_end = output[1].reshape((0, -3)).asnumpy()\n",
    "\n",
    "    for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "        mx_results[example_id].append(PredResult(start=start, end=end))\n",
    "\n",
    "epoch_toc = time.time()\n",
    "print('Time cost={:.2f} s, Thoughput={:.2f} samples/s'.format(\n",
    "    epoch_toc - epoch_tic, total_num/(epoch_toc - epoch_tic)))\n",
    "\n",
    "qa_utils.predict(dataset, mx_results, vocab, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark TVM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmark shows the mean inference time of TVM and MXNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM mean inference time: 102.92 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = np.random.uniform(size=(1, max_seq_length)).astype('float32')\n",
    "token_types = np.random.uniform(size=(1, max_seq_length)).astype('float32')\n",
    "valid_length = np.asarray([max_seq_length]).astype('float32')\n",
    "\n",
    "ftimer = ex.module.time_evaluator(\"run\", tvm_ctx, number=20, min_repeat_ms=1000)\n",
    "prof_res = np.array(ftimer().results) * 1000  # convert to millisecond\n",
    "print(\"TVM mean inference time: %.2f ms\" % np.mean(prof_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MXNet mean inference time: 85.82 ms\n"
     ]
    }
   ],
   "source": [
    "inputs_nd = mx.nd.array(inputs)\n",
    "token_types_nd = mx.nd.array(token_types)\n",
    "valid_length_nd = mx.nd.array(valid_length)\n",
    "mx_out = net(inputs_nd, token_types_nd, valid_length_nd)\n",
    "mx_out.wait_to_read()\n",
    "\n",
    "min_repeat_ms = 1000\n",
    "number = 20\n",
    "while True:\n",
    "    beg = time.time()\n",
    "    for _ in range(number):\n",
    "        mx_out = net(inputs_nd, token_types_nd, valid_length_nd)\n",
    "        mx_out.wait_to_read()\n",
    "    end = time.time()\n",
    "    lat = (end - beg) * 1e3\n",
    "    if lat >= min_repeat_ms:\n",
    "        break\n",
    "    number = int(max(min_repeat_ms / (lat / number) + 1, number * 1.618))\n",
    "print('MXNet mean inference time: %.2f ms' % (lat / number))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
